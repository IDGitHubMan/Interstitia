{% extends "base.html" %}
{% load static %}
{% block title %}
    Trends
{% endblock title %}
{% block extraSrcs %}
    <link rel="stylesheet"
          type="text/css"
          href="{% static 'CSS/projects/Trends.css' %}"/>
{% endblock extraSrcs %}
{% block content %}
    <div class="title">
        <img src="{% static 'images/Trends/Trend.png' %}" alt="Trends home page." width="100%" height="auto"/>
        <div class="headText">
            <h2>Trends</h2>
            <p>
                Google is a search engine so widely used and popular that it gained a verb form. There are other search engines of course, but they don't even come close to it's usage. So, why not explore that? See what the top searches are worldwide? And then, visualize that somehow in an interesting way?
            </p>
        </div>
    </div>
    <div class="pImg">
        <div class="sideText">
            <p>
            This project was inspired by Google Trends, which allows one to view all google search data from 2004 onwards. This record of now (as I'm writing) 18 years is massive, considering that Google (at least now) processes over 8 billion searches <em>daily</em>. That would essentially translate to each person alive on the planet googling something a little more than once a day. Google Trends handles this rather nicely. It presents you with a searchbox, where you search your term of interest. From there, you can easily apply a wide range of filters and options based on location, time, or type of search to explore. You could also compare 5 terms at once. What is incredible is how this dataset can be used to also chart history, as the web is where we go to talk about things, whether to learn about, discuss or share them.
        </p>
        <p>
            As if that weren't nice enough, Trends also records data from the day - both suummarily and in real time. One need only select a country and category, and voila! A list of the top 20 or so trends as of that moment. Additionally, it compiles searches that led to the same webpages, so as to prevent too much spread. So if some people serached for 'soccer' and some for 'FIFA' or 'World Cup', but all were lloking to see the latest results for the preliminaries, Trends automatically categorizes them together. Due to I believe the sheer mass of searches, it doesn't update automatically, but every time you reload the page, it pulls new data. Of course, you're not likely to see changes every second, but every minute or so, the whole list could be completely different. And to think it does this for every country! And I'm not even mentioning their year in search projects, insights, and stories. It's interactivity and usefulness is a high standard.
        </p>
    </div>
    <div class="captionedImg">
        <img class="side"
             src="{% static 'images/Trends/Screen Shot 2022-04-26 at 11.45.38 AM.png' %}"
             width="100%"
             height="auto"
             alt="Realtime trends for US as of 04/26/2022 at 11:45 AM EST"/>
        <p>
            Realtime trends for US as of 04/26/2022 at 11:45 AM EST
        </p>
    </div>
</div>
<div class="pImg">
    <div class="captionedImg">
        <img class="side"
             width="100%"
             height="auto"
             alt="Google Trends Mapping"
             alt="Google Trends' lame maps."
             src="{% static 'images/Trends/Maps.png' %}"/>
        <p>
            Google Trends' mapping
        </p>
    </div>
    <div class="sideText">
        <p>
            For all its worth though, I belive that Trends is lacking in one major aspect: The visualization of their data. If you wanted to view the interest over time of something, a line graph would be great. But considering the financing and power that google has, one would think they could do more than just a simple line, with point hovering. Maybe some kind of zoom, or a more detailed view of the line. Lists are useful, but if you can have up to 500 datapoints, they're not really helpful for users, and rather more cluttered and confusing. In addition, there is no engaging kind of animation, sound, or interactivity which, might be only my opinion, but would make it all the nicer. Imagine having the line be drawn out over time rather than simply plopped there.
        </p>
        <p>
            But my biggest gripe has to be the maps. One would think that with Google, owner of the interactive and ever-useful Google Maps, there could be some integration of the two. Having an interactive, draggable, zoomable map, with clear indicators of region or country and relevant data. But with Trends, there are only static images that have popups and links to other pages with smaller maps. The final map only does a little magnifying ring, and that only if you hover over a point! And there is no mapping of the realtime or daily trends, even though it would make for a much simpler and easier means of comparing trends and interest in events across the globe.
        </p>
    </div>
</div>
<div class="midBig"
     style="background-image: url('{% static 'images/Trends/Map.png' %}'); background-size: 100%"
     height="windowHeight">
    <h2>So I decided to do one for myself.</h2>
    <p>
    It was tough, but fun. Google should definitely hire me. Or at least not <a>block</a>my program.
</p>
</div>
<p>
    I decided to write a program. But what to write it in? Processing is one of my favorite programming languages because it's oriented around visuals and interactivity, much more so than others. Chance are you saw the interactives on the home page. Those were made with Processing's JS equivalent, p5js. But, while Processing can access web pages, it can't do it very quickly or cleanly. Visiting and reading through a page takes time and micromanaging that could lead to slow and inaccurate datasets. Python, on the other hand, could visit these pages and process data in seconds, compared to processing's minutes, but lacked the visuals of Processing. So, I made the obvious choice. My program would be two pieces, one that trawls the Trends pages and creates a basic tabulated file in Python, and one to read through that table and visualize accordingly in processing. The visualization would be a map that shows top searches for each country.
</p>
<p class="c4">
    <span class="c0">Now, the first part. Python. There are 195 countries in the world. And the Google Trends data is organized according to country and category. These values are stored in the URL as parameters. I could easily make a python script that iterates through the country codes and categories, injects them into a url, and then makes a request to the web page. So, after attempting (and failing) to scrape the country codes from a Wikipedia site, I downloaded a CSV (thanks Kaggle) that had not only country codes, but also their longitude and latitude, which would be useful for the processing later. The code checks each row from the file to get a country code, which it inserts into the url. There is a second loop going over the strings b, e, m, t, s, h, and all to select for categories.</span>
</p>
<p class="c3">
    <span class="c0"></span>
</p>
<p class="c4">
    <span>The first problem arrives at this point when I get to accessing web pages. I'd been using a</span><span>module</span><span>&nbsp;called BeautifulSoup to read through the HTML, and it has a function that basically finds</span><span>HTML elements</span><span>&nbsp;based on your conditions. So, after inspecting the web page for the</span><span>containers</span><span>&nbsp;with the data, I used it to find them. And they were nowhere to be found. At all. I found out that this was because the trends website uses Angular to render the data from another site, under the hood. And basic requests aren't patient or complex enough to make these </span><span>secondary or tertiary</span><span>&nbsp;calls unless I found the secondary URL. These requests are distinct not only to country, and category but also actual </span><span class="c2">results</span><span class="c0">. I'd have to find the URL every time, and requests can't access them. So... I need another web scraper. I decided on Selenium, which retrieved all the elements by actually running chrome or safari to retrieve the page's data. SO, I had my data. Or did I?</span>
</p>
<p class="c3">
    <span class="c0"></span>
</p>
<p class="c4">
    <span>Problem number two. Selenium got the data, but it's formatting was kind of... nasty. For one thing, the boxes with each search have multiple searches. So, if I wanted the top search I'd get a list of 5 items rather than just the one. Not to mention the fact that this is HTML, so the values are </span><span>nested</span><span class="c0">&nbsp;more than three layers down. That's right. It's crazier than Inception with HTML. This issue meshed with the previous one. I still needed to find the nested request. Javascript has a nice function that lists all requests made by a webpage, and Selenium has a nicer function that lets you run javascript on a webpage. So, put those together, look for only the requests I want, and there. I have the data.</span>
</p>
<p class="c3">
    <span class="c0"></span>
</p>
<p class="c4">
    <span class="c0">Problem 3. Way back when, if someone wanted to steal data from a website, they could actually send a ridiculous amount of traffic to temporarily disable it, and in the resulting mess, get the info they wanted. This was called a DDoS, or denial of service attack. Now, Google and other sites automatically block those attacks. Google thought my program was one of these and was blocking the data. But I realized that a lot of the pages I was visiting were either completely blank or just said. Yeah. This means the Trends Data wasn't as thorough as I thought it was. More on that later. As a workaround, I edited the CSV I was pulling country codes from so that it didn't include countries that lacked information. Now, I can get away with spamming Trends for info, and get the secondary request that I needed.</span>
</p>
<p class="c3">
    <span class="c0"></span>
</p>
<p class="c4">
    <span>Problem four. The nested request doesn't lead to an actual document. Instead, it downloads a text file that is a mess of text and numbers with brackets thrown in. Thankfully, I knew that the files were actually JSON. JSON is a way of storing data that is often used by applications and programs to communicate with each other. The format stores data in key-value</span><span>&nbsp;pairs</span><span class="c0">. So if I got it to work, I could actually just pull information directly from the JSON. Or could I?</span>
</p>
<p class="c3">
    <span class="c0"></span>
</p>
<p class="c4">
    <span>Problem 5, is by far the most annoying one. This one was relatively simple to solve, which made it far worse. Python has a function that automatically converts JSON into a more accessible format, so I wouldn't have to search through the entire file for one value. I slap that onto the downloaded file and get one of the most unhelpful error messages one can get. Google helps me to figure out that something is wrong with the JSON at the first character, so Python is giving up immediately. The first character in JSON must be an opening curly brace. I check the file, and indeed, the first four characters are this nonsense. So I just trim them off and then try again. Still an error. I spend hours before I realize there is a hidden </span><span>newline</span><span class="c0">&nbsp;character, which tells the word processor/computer to add spaces until it moves onto a new line. I delete the entire first line and FINALLY have the JSON working. After setting up more automation to delete and edit the downloaded file as needed, I can take the values I need, and write them into a CSV file.</span>
</p>
<p class="c3">
    <span class="c0"></span>
</p>
<p class="c4">
    <span class="c0">So, the code explained in a nutshell:</span>
</p>
<ol class="c6 lst-kix_7jy79u2lddhp-0 start" start="1">
    <li class="c1 li-bullet-0">
        <span class="c0">Get all the stuff I need for the code to work.</span>
    </li>
    <li class="c1 li-bullet-0">
        <span class="c0">Activate the chrome driver</span>
    </li>
    <li class="c1 li-bullet-0">
        <span class="c0">Open the CSV with country codes and stuff</span>
    </li>
    <li class="c1 li-bullet-0">
        <span class="c0">Create a list that'll store the data for the output tabulated file</span>
    </li>
    <li class="c1 li-bullet-0">
        <span class="c0">Now the complicated part. For every row within the CSV, and for every category, I create a URL and send it for the chrome driver to visit, giving the page a second to fully load.</span>
    </li>
    <li class="c1 li-bullet-0">
        <span class="c0">Then I use selenium to call a js function that produces a list of all secondary requests made by the page.</span>
    </li>
    <li class="c1 li-bullet-0">
        <span class="c0">Sort through the requests for the page until I find the top search result</span>
    </li>
    <li class="c1 li-bullet-0">
        <span class="c0">Make the secondary request and let the file download</span>
    </li>
    <li class="c1 li-bullet-0">
        <span class="c0">Open the file and remove that nagging first line</span>
    </li>
    <li class="c1 li-bullet-0">
        <span class="c0">Make it easy to look through the data within the file</span>
    </li>
    <li class="c1 li-bullet-0">
        <span class="c0">Pull data from the now readable JSON and longitudes/latitudes from the first CSV, adding it to the list I created</span>
    </li>
    <li class="c1 li-bullet-0">
        <span class="c0">Close and delete the file so my computer isn't suddenly swamped with text files.</span>
    </li>
    <li class="c1 li-bullet-0">
        <span class="c0">Break out of the loop so I only get the top result</span>
    </li>
    <li class="c1 li-bullet-0">
        <span class="c0">Write the list to a new file, which can be accessed by processing.</span>
    </li>
</ol>
<p class="c3">
    <span class="c0"></span>
</p>
<p class="c4">
    <span class="c0">Now, the fun part. I already had a file that displayed a map, so I would use that to start off. It turns out I can set up Processing to run the python file. The first time runs the file 30 times a second, so I have a ridiculous amount of chrome windows opening, spamming websites, and eventually freezing my whole computer. So, I make it run every 20 minutes because the Python file takes about 15ish minutes to run, and I want it to have time to cool a bit. Whether or not the python is running, processing opens the CSV and goes through it line by line, creating text boxes that are supposed to be mapped to the centers of each country detailing the number one search. The visuals are off, but it does the job! There's probably a mistake with the translation of longitude and latitude to placement on the processing grid, but it's working for now! The scaling part is relatively easy, making the python also retrieve the indexes for sizing is light work.</span>
</p>
<p class="c3">
    <span class="c0"></span>
</p>
<p class="c4">
    <span class="c0">This project is rudimentary. Hopefully, it could expand into something bigger and better. For instance, maybe including other search engine data, or incorporating a more interactive model. As an end goal, having a live and fully active version of the program online would be the best.</span>
</p>
<p class="c3">
    <span class="c0"></span>
</p>
<p class="c4">
    <span class="c0">Lastly, the dearth of data that was apparent can communicate many things. I'm not exactly sure which interpretation of it is best. It could be a notice of the fact that many of the world's countries are still developing. It could be that Google is not as popular as it appears to be. It could be a lack of interest in cataloging data for certain countries, or maybe these countries themselves do not want to be cataloged. Whatever it may be, it is fair to note that out of 195 countries, less than a quarter have data, and only some of those have real-time data. Take from that what you will, but I won't let it be something that I get too lost in because that wasn't the point. This was an exploration that happened to call to attention something unique. I would like to have more of these explorations in the future and see what can be gotten from them.</span>
</p>
{% endblock content %}
