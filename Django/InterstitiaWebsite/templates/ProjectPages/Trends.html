<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol.lst-kix_7jy79u2lddhp-8{list-style-type:none}ol.lst-kix_7jy79u2lddhp-4.start{counter-reset:lst-ctn-kix_7jy79u2lddhp-4 0}.lst-kix_7jy79u2lddhp-4>li{counter-increment:lst-ctn-kix_7jy79u2lddhp-4}ol.lst-kix_7jy79u2lddhp-2.start{counter-reset:lst-ctn-kix_7jy79u2lddhp-2 0}.lst-kix_7jy79u2lddhp-1>li{counter-increment:lst-ctn-kix_7jy79u2lddhp-1}ol.lst-kix_7jy79u2lddhp-6.start{counter-reset:lst-ctn-kix_7jy79u2lddhp-6 0}ol.lst-kix_7jy79u2lddhp-0.start{counter-reset:lst-ctn-kix_7jy79u2lddhp-0 0}ol.lst-kix_7jy79u2lddhp-2{list-style-type:none}.lst-kix_7jy79u2lddhp-1>li:before{content:"" counter(lst-ctn-kix_7jy79u2lddhp-1,lower-latin) ". "}ol.lst-kix_7jy79u2lddhp-3{list-style-type:none}ol.lst-kix_7jy79u2lddhp-0{list-style-type:none}ol.lst-kix_7jy79u2lddhp-8.start{counter-reset:lst-ctn-kix_7jy79u2lddhp-8 0}ol.lst-kix_7jy79u2lddhp-1{list-style-type:none}ol.lst-kix_7jy79u2lddhp-6{list-style-type:none}.lst-kix_7jy79u2lddhp-7>li{counter-increment:lst-ctn-kix_7jy79u2lddhp-7}.lst-kix_7jy79u2lddhp-2>li:before{content:"" counter(lst-ctn-kix_7jy79u2lddhp-2,lower-roman) ". "}.lst-kix_7jy79u2lddhp-3>li:before{content:"" counter(lst-ctn-kix_7jy79u2lddhp-3,decimal) ". "}ol.lst-kix_7jy79u2lddhp-7{list-style-type:none}ol.lst-kix_7jy79u2lddhp-4{list-style-type:none}ol.lst-kix_7jy79u2lddhp-5{list-style-type:none}ol.lst-kix_7jy79u2lddhp-3.start{counter-reset:lst-ctn-kix_7jy79u2lddhp-3 0}.lst-kix_7jy79u2lddhp-4>li:before{content:"" counter(lst-ctn-kix_7jy79u2lddhp-4,lower-latin) ". "}.lst-kix_7jy79u2lddhp-5>li:before{content:"" counter(lst-ctn-kix_7jy79u2lddhp-5,lower-roman) ". "}.lst-kix_7jy79u2lddhp-0>li{counter-increment:lst-ctn-kix_7jy79u2lddhp-0}.lst-kix_7jy79u2lddhp-0>li:before{content:"" counter(lst-ctn-kix_7jy79u2lddhp-0,decimal) ". "}.lst-kix_7jy79u2lddhp-8>li:before{content:"" counter(lst-ctn-kix_7jy79u2lddhp-8,lower-roman) ". "}.lst-kix_7jy79u2lddhp-3>li{counter-increment:lst-ctn-kix_7jy79u2lddhp-3}ol.lst-kix_7jy79u2lddhp-5.start{counter-reset:lst-ctn-kix_7jy79u2lddhp-5 0}.lst-kix_7jy79u2lddhp-6>li:before{content:"" counter(lst-ctn-kix_7jy79u2lddhp-6,decimal) ". "}.lst-kix_7jy79u2lddhp-7>li:before{content:"" counter(lst-ctn-kix_7jy79u2lddhp-7,lower-latin) ". "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ol.lst-kix_7jy79u2lddhp-1.start{counter-reset:lst-ctn-kix_7jy79u2lddhp-1 0}.lst-kix_7jy79u2lddhp-6>li{counter-increment:lst-ctn-kix_7jy79u2lddhp-6}.lst-kix_7jy79u2lddhp-5>li{counter-increment:lst-ctn-kix_7jy79u2lddhp-5}.lst-kix_7jy79u2lddhp-8>li{counter-increment:lst-ctn-kix_7jy79u2lddhp-8}ol.lst-kix_7jy79u2lddhp-7.start{counter-reset:lst-ctn-kix_7jy79u2lddhp-7 0}.lst-kix_7jy79u2lddhp-2>li{counter-increment:lst-ctn-kix_7jy79u2lddhp-2}ol{margin:0;padding:0}table td,table th{padding:0}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c5{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c6{padding:0;margin:0}.c4{margin-left:36pt;padding-left:0pt}.c3{font-style:italic}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c5"><p class="c2"><span class="c0">Google. A search engine so widely used and popular that it gained a verb form. Wanted to know the distance between two celestial bodies? Google it. How many people are in a country? Google it. Wondering What TNT stands for? It&rsquo;s chemical formula? Google that, and get the second too. There are other search engines of course. Bing, Yahoo. But they don&rsquo;t even come close to Google usage. So, why not explore that? See what the top searches are worldwide? And then, visualize that somehow in a sort of interesting way?</span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span>This project was inspired by Google Trends, which allows one to view all google search data from 2004. This expansive dataset also lets you look for terms and locations, and view interest as gauged by the concentration of searches over time. However, the trends website is also capable of generating pages that detail search trends </span><span class="c3">in real-time</span><span class="c0">.</span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span>I wanted to visualize this somehow other than graphs and charts. As a programmer, I decided I&rsquo;d write a program. Processing is one of my favorite programming languages because it&#39;s oriented around visuals and interactivity, much more so than others. But, while it can access web pages, it can&rsquo;t do it very </span><span class="c3">quickly </span><span>or </span><span class="c3">cleanly</span><span class="c0">. Visiting and reading through a page takes time and micromanaging that could lead to slow and inaccurate datasets. Python, on the other hand, could visit these pages and pull data in seconds, compared to processing&rsquo;s minutes, but lacked the visuals of Processing. So, I made the obvious choice. My &ldquo;program&rdquo; would be two, one that trawls the Trends pages and creates a basic tabulated file in Python, and one to read through that table and visualize accordingly in processing. The visualization would be a map that shows top searches for each country.</span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">Now, the first part. Python. There are 195 countries in the world. And the Google Trends data is organized according to country and category. These values are stored in the URL as parameters. I could easily make a python script that iterates through the country codes and categories, injects them into a url, and then makes a request to the web page. So, after attempting (and failing) to scrape the country codes from a Wikipedia site, I downloaded a CSV (thanks Kaggle) that had not only country codes, but also their longitude and latitude, which would be useful for the processing later. The code checks each row from the file to get a country code, which it inserts into the url. There is a second loop going over the strings b, e, m, t, s, h, and all to select for categories.</span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span>The first problem arrives at this point when I get to accessing web pages. I&rsquo;d been using a </span><span>module</span><span>&nbsp;called BeautifulSoup to read through the HTML, and it has a function that basically finds </span><span>HTML elements</span><span>&nbsp;based on your conditions. So, after inspecting the web page for the </span><span>containers</span><span>&nbsp;with the data, I used it to find them. And they were nowhere to be found. At all. I found out that this was because the trends website uses Angular to render the data from another site, under the hood. And basic requests aren&#39;t patient or complex enough to make these </span><span>secondary or tertiary</span><span>&nbsp;calls unless I found the secondary URL. These requests are distinct not only to country, and category but also actual </span><span class="c3">results</span><span class="c0">. I&rsquo;d have to find the URL every time, and requests can&rsquo;t access them. So&hellip; I need another web scraper. I decided on Selenium, which retrieved all the elements by actually running chrome or safari to retrieve the page&rsquo;s data. SO, I had my data. Or did I?</span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span>Problem number two. Selenium got the data, but it&rsquo;s formatting was kind of&hellip; nasty. For one thing, the boxes with each search have multiple searches. So, if I wanted the top search I&rsquo;d get a list of 5 items rather than just the one. Not to mention the fact that this is HTML, so the values are </span><span>nested</span><span class="c0">&nbsp;more than three layers down. That&rsquo;s right. It&rsquo;s crazier than Inception with HTML. This issue meshed with the previous one. I still needed to find the nested request. Javascript has a nice function that lists all requests made by a webpage, and Selenium has a nicer function that lets you run javascript on a webpage. So, put those together, look for only the requests I want, and there. I have the data.</span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">Problem 3. Way back when, if someone wanted to steal data from a website, they could actually send a ridiculous amount of traffic to temporarily disable it, and in the resulting mess, get the info they wanted. This was called a DDoS, or denial of service attack. Now, Google and other sites automatically block those attacks. Google thought my program was one of these and was blocking the data. But I realized that a lot of the pages I was visiting were either completely blank or just said&hellip;. Yeah. This means the Trends Data wasn&rsquo;t as thorough as I thought it was. More on that later. As a workaround, I edited the CSV I was pulling country codes from so that it didn&rsquo;t include countries that lacked information. Now, I can get away with spamming Trends for info, and get the secondary request that I needed.</span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span>Problem four. The nested request doesn&rsquo;t lead to an actual document. Instead, it downloads a text file that is a mess of text and numbers with brackets thrown in. Thankfully, I knew that the files were actually JSON. JSON is a way of storing data that is often used by applications and programs to communicate with each other. The format stores data in key-value</span><span>&nbsp;pairs</span><span class="c0">. So if I got it to work, I could actually just pull information directly from the JSON. Or could I?</span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span>Problem 5, is by far the most annoying one. This one was relatively simple to solve, which made it far worse. Python has a function that automatically converts JSON into a more accessible format, so I wouldn&rsquo;t have to search through the entire file for one value. I slap that onto the downloaded file and get one of the most unhelpful error messages one can get. Google helps me to figure out that something is wrong with the JSON at the first character, so Python is giving up immediately. The first character in JSON must be an opening curly brace. I check the file, and indeed, the first four characters are this nonsense. So I just trim them off and then try again. Still an error. I spend hours before I realize there is a hidden </span><span>newline</span><span class="c0">&nbsp;character, which tells the word processor/computer to add spaces until it moves onto a new line. I delete the entire first line and FINALLY have the JSON working. After setting up more automation to delete and edit the downloaded file as needed, I can take the values I need, and write them into a CSV file.</span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">So, the code explained in a nutshell:</span></p><ol class="c6 lst-kix_7jy79u2lddhp-0 start" start="1"><li class="c2 c4 li-bullet-0"><span class="c0">Get all the stuff I need for the code to work.</span></li><li class="c2 c4 li-bullet-0"><span class="c0">Activate the chrome driver</span></li><li class="c2 c4 li-bullet-0"><span class="c0">Open the CSV with country codes and stuff</span></li><li class="c2 c4 li-bullet-0"><span class="c0">Create a list that&rsquo;ll store the data for the output tabulated file</span></li><li class="c2 c4 li-bullet-0"><span class="c0">Now the complicated part. For every row within the CSV, and for every category, I create a URL and send it for the chrome driver to visit, giving the page a second to fully load.</span></li><li class="c2 c4 li-bullet-0"><span class="c0">Then I use selenium to call a js function that produces a list of all secondary requests made by the page.</span></li><li class="c2 c4 li-bullet-0"><span class="c0">Sort through the requests for the page until I find the top search result</span></li><li class="c2 c4 li-bullet-0"><span class="c0">Make the secondary request and let the file download</span></li><li class="c2 c4 li-bullet-0"><span class="c0">Open the file and remove that nagging first line</span></li><li class="c2 c4 li-bullet-0"><span class="c0">Make it easy to look through the data within the file</span></li><li class="c2 c4 li-bullet-0"><span class="c0">Pull data from the now readable JSON and longitudes/latitudes from the first CSV, adding it to the list I created</span></li><li class="c2 c4 li-bullet-0"><span class="c0">Close and delete the file so my computer isn&rsquo;t suddenly swamped with text files.</span></li><li class="c2 c4 li-bullet-0"><span class="c0">Break out of the loop so I only get the top result</span></li><li class="c2 c4 li-bullet-0"><span class="c0">Write the list to a new file, which can be accessed by processing.</span></li></ol><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">Now, the fun part. I already had a file that displayed a map, so I would use that to start off. It turns out I can set up Processing to run the python file. The first time runs the file 30 times a second, so I have a ridiculous amount of chrome windows opening, spamming websites, and eventually freezing my whole computer. So, I make it run every 20 minutes because the Python file takes about 15ish minutes to run, and I want it to have time to cool a bit. Whether or not the python is running, processing opens the CSV and goes through it line by line, creating text boxes that are supposed to be mapped to the centers of each country detailing the number one search. The visuals are off, but it does the job! There&rsquo;s probably a mistake with the translation of longitude and latitude to placement on the processing grid, but it&rsquo;s working for now! The scaling part is relatively easy, making the python also retrieve the indexes for sizing is light work.</span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">This project is rudimentary. Hopefully, it could expand into something bigger and better. For instance, maybe including other search engine data, or incorporating a more interactive model. As an end goal, having a live and fully active version of the program online would be the best.</span></p><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">Lastly, the dearth of data that was apparent can communicate many things. I&rsquo;m not exactly sure which interpretation of it is best. It could be a notice of the fact that many of the world&rsquo;s countries are still developing. It could be that Google is not as popular as it appears to be. It could be a lack of interest in cataloging data for certain countries, or maybe these countries themselves do not want to be cataloged. Whatever it may be, it is fair to note that out of 195 countries, less than a quarter have data, and only some of those have real-time data. Take from that what you will, but I won&rsquo;t let it be something that I get too lost in because that wasn&rsquo;t the point. This was an exploration that happened to call to attention something unique. I would like to have more of these explorations in the future and see what can be gotten from them.</span></p></body></html>